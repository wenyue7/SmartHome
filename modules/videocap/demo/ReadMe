=============================================== <基础知识> ===============================================
FFmpeg 基础知识
基础术语
	容器(Container)
​		一种文件格式，比如flv，mkv等。包含下面5种流以及文件头信息。
	流(Stream)
​		一种视频数据信息的传输方式，5种流：音频，视频，字幕，附件，数据。
	帧(Frame)
​		帧代表一幅静止的图像，分为I帧，P帧，B帧。
	编解码器(Codec)
​		是对视频进行压缩或者解压缩，CODEC = COde（编码） +DECode（解码）。
	复用/解复用(mux/demux)
​		把不同的流按照某种容器的规则放入容器，这种行为叫做复用（mux）。
	​	把不同的流从某种容器中解析出来，这种行为叫做解复用(demux)。
	帧率
​		帧率也叫帧频率，帧率是视频文件中每一秒的帧数，肉眼想看到连续移动图像至少需要15帧。
	码率
​		比特率(也叫码率，数据率)是一个确定整体视频/音频质量的参数，秒为单位处理的字节数，码率和视频质量成正比，在视频文件中中比特率用bps来表达。

简介
​	FFmpeg的名称来自MPEG视频编码标准，前面的“FF”代表“Fast Forward”，FFmpeg是一套可以用来记录、转换数字音频、视频， 并能将其转化为流的开源计算机程序。 可以轻易地实现多种视频格式之间的相互转换。包括如下几个部分：
	libavformat：用于各种音视频封装格式的生成和解析，包括获取解码所需信息以生成解码上下文结构和读取音视频帧等功能，包含demuxers和muxer库。
	libavcodec：用于各种类型声音/图像编解码。
	libavutil：包含一些公共的工具函数。
	libswscale：用于视频场景比例缩放、色彩映射转换。
	libpostproc：用于后期效果处理。
	ffmpeg：是一个命令行工具，用来对视频文件转换格式，也支持对电视卡实时编码。
	ffsever：是一个HTTP多媒体实时广播流服务器，支持时光平移。
	ffplay：是一个简单的播放器，使用ffmpeg 库解析和解码，通过SDL显示。
	ffprobe：收集多媒体文件或流的信息，并以人和机器可读的方式输出。
参考资料
《FFmpeg基础》

=============================================== <相关资源> ===============================================
FFMPEG中最关键的结构体之间的关系:
https://blog.csdn.net/leixiaohua1020/article/details/11693997

视音频编解码技术零基础学习方法:
https://blog.csdn.net/leixiaohua1020/article/details/18893769

雷神Git,里边有好多demo:
https://github.com/leixiaohua1020

ffmpeg编码:
https://blog.csdn.net/leixiaohua1020/article/details/25430425

ffmpeg解码:

最简单的基于FFmpeg的封装格式处理：视音频复用器（muxer):
https://blog.csdn.net/leixiaohua1020/article/details/39802913/

FFmpeg源代码结构图 - 编码:
https://blog.csdn.net/leixiaohua1020/article/details/44226355

最简单的基于FFmpeg的推流器（以推送RTMP为例）:
https://blog.csdn.net/leixiaohua1020/article/details/39803457

最简单的基于FFMPEG的推流器附件：收流器:
https://blog.csdn.net/leixiaohua1020/article/details/46890487

关于DTS和PTS:  (https://www.jianshu.com/p/cc58153ac98c)
    FFmpeg里有两种时间戳：DTS（Decoding Time Stamp）和PTS（Presentation Time Stamp）。 
顾名思义，前者是解码的时间，后者是显示的时间。要仔细理解这两个概念，需要先了解FFmpeg中的packet和frame的概念。
    FFmpeg中用AVPacket结构体来描述解码前或编码后的压缩包，用AVFrame结构体来描述解码后或编码前的信号帧。 
对于视频来说，AVFrame就是视频的一帧图像。这帧图像什么时候显示给用户，就取决于它的PTS。DTS是AVPacket里的一个
成员，表示这个压缩包应该什么时候被解码。如果视频里各帧的编码是按输入顺序（也就是显示顺序）依次进行的，那么解码和
显示时间应该是一致的。可事实上，在大多数编解码标准（如H.264或HEVC）中，编码顺序和输入顺序并不一致。 于是才会
需要PTS和DTS这两种不同的时间戳。
需要改变帧率的话需要修改AVStream->time_base.den

=============================================== <推流拉流> ===============================================
采用srs服务,该服务延迟要比nginx-rtmp小很多,不排除配置参数的影响,但srs服务很易上手,推荐使用.

使用srs服务时,使用ffplay拉流要比使用vlc播放器拉流延迟低很多
使用nginx-rtmp服务时,使用ffplay拉流和使用vlc播放器拉流延迟相差无几

srs的配置方法及使用教程均可以在github上找到:
https://github.com/ossrs/srs
https://github.com/ossrs/srs/wiki/v3_CN_Home

使用rtmp或hls服务时,根据github上的教程修改conf下的配置文件:
推流脚本:  for((;;)); do   ffmpeg -re -i /dev/video1 -vcodec h264 -f flv rtmp://192.168.1.5/live/livestream;    sleep 1;  done
拉流脚本:  ffplay -i rtmp://192.168.1.5/live/livestream
rtmp实时性要优于hls实时性,这里使用了rtmp